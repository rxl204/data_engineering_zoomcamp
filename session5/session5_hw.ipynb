{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68b02a2-5206-4eea-9776-a05613d2830b",
   "metadata": {},
   "source": [
    "#### Test running pyspark + homework"
   ]
  },
  {
   "cell_type": "raw",
   "id": "974f4cd0-7a24-4329-97ea-b64d69015f58",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set JAVA_HOME environment variable\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/local/Cellar/openjdk@11/11.0.22\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0875b8-8305-41a9-9421-407c3f447e56",
   "metadata": {},
   "source": [
    "##### Q2\n",
    "FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8187d4-1583-4907-b3e9-6b4f9f23e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B03162|2019-10-02 17:39:02|2019-10-02 18:42:21|         201|          71|   NULL|                B03162|\n",
      "|              B02293|2019-10-03 12:24:26|2019-10-03 13:25:24|          65|         222|   NULL|                B02293|\n",
      "|              B02401|2019-10-06 22:32:17|2019-10-06 22:37:05|         264|         256|   NULL|                B02401|\n",
      "|     B00021         |2019-10-01 23:08:47|2019-10-01 23:15:57|          56|          82|   NULL|       B00021         |\n",
      "|              B02838|2019-10-01 08:19:52|2019-10-01 08:33:04|         264|          26|   NULL|                B02838|\n",
      "|              B03107|2019-10-01 08:09:51|2019-10-01 08:40:23|          69|         259|   NULL|                B03107|\n",
      "|              B00318|2019-10-05 22:05:39|2019-10-05 22:05:43|         264|          63|   NULL|                B00318|\n",
      "|              B02120|2019-10-07 18:12:00|2019-10-07 19:00:43|         264|         264|   NULL|                B02120|\n",
      "|              B02715|2019-10-05 09:37:19|2019-10-05 10:35:00|         132|         251|   NULL|                B02932|\n",
      "|              B01417|2019-10-04 12:29:56|2019-10-04 13:57:02|         264|         264|   NULL|                B01417|\n",
      "|              B01231|2019-10-07 18:17:44|2019-10-07 18:32:30|         264|         217|   NULL|                B02918|\n",
      "|              B01231|2019-10-04 14:37:01|2019-10-04 14:47:47|         264|         217|   NULL|                B02800|\n",
      "|              B00256|2019-10-04 10:26:09|2019-10-04 10:57:50|         264|         264|   NULL|                B00256|\n",
      "|              B02531|2019-10-04 13:00:00|2019-10-04 14:00:00|           9|          92|   NULL|                B02531|\n",
      "|              B01051|2019-10-05 03:39:28|2019-10-05 03:48:43|         264|          78|   NULL|                B01051|\n",
      "|              B03069|2019-10-03 21:15:00|2019-10-03 21:57:00|         264|         264|   NULL|                B03069|\n",
      "|              B00837|2019-10-01 20:31:18|2019-10-01 21:04:15|         264|         264|   NULL|                B00837|\n",
      "|              B02849|2019-10-01 14:49:26|2019-10-01 15:00:24|         264|          33|   NULL|                B02849|\n",
      "|              B01710|2019-10-06 10:30:27|2019-10-06 10:54:57|         264|         264|   NULL|                B01710|\n",
      "|              B02550|2019-10-02 09:56:43|2019-10-02 10:11:46|         264|          82|   NULL|                B02550|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code presents homework for question 2\n",
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')\n",
    "\n",
    "# Define a custom schema for the DataFrame with specific fields and data types.\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])\n",
    "\n",
    "# Re-reads the CSV file with the specified schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')\n",
    "\n",
    "# Repartitions the DataFrame\n",
    "df = df.repartition(6)\n",
    "\n",
    "# Write the DataFrame to Parquet format\n",
    "df.write.parquet('fhv/2019/10/')\n",
    "\n",
    "# Read the Parquet data back into a DataFrame\n",
    "df = spark.read.parquet('fhv/2019/10/')\n",
    "\n",
    "# Print the schema\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e0825-cbbe-4b35-ba3c-1bc5c1459a37",
   "metadata": {},
   "source": [
    "##### Q3\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcbde7f-04f8-4b9a-a54f-02883d240192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachel/opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code presents homework for question 3\n",
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read Parquet data into a DataFrame\n",
    "df = spark.read.parquet('fhv/2019/10/')\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.registerTempTable('fhv_2019_10')\n",
    "\n",
    "# Execute a SQL query using Spark SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhv_2019_10\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2019-10-15';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0450845-e643-4c84-9324-4f21e4c5035f",
   "metadata": {},
   "source": [
    "#### Q4\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e798eb94-1ce3-43ec-a4ac-5361e5cce26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|pickup_date|max(duration_hours)|\n",
      "+-----------+-------------------+\n",
      "| 2019-10-28|           631152.5|\n",
      "| 2019-10-11|           631152.5|\n",
      "| 2019-10-31|  87672.44083333333|\n",
      "| 2019-10-01|  70128.02805555555|\n",
      "| 2019-10-17|             8794.0|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "   # This code presents homework for question 4\n",
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read Parquet data into a DataFrame\n",
    "df = spark.read.parquet('fhv/2019/10/')\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.registerTempTable('fhv_2019_10')\n",
    "\n",
    "# Perform DataFrame transformations and aggregations by adding two new columns duration_hours and pickup_date\n",
    "df \\\n",
    "    .withColumn('duration_hours', (df.dropOff_datetime.cast('long') - df.pickup_datetime.cast('long')) / 3600) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration_hours') \\\n",
    "    .orderBy('max(duration_hours)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46599701-9ba2-486b-850c-6b699127b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|count(PULocationID)|                Zone|\n",
      "+-------------------+--------------------+\n",
      "|                  1|         Jamaica Bay|\n",
      "|                  2|Governor's Island...|\n",
      "|                  5| Green-Wood Cemetery|\n",
      "|                  8|       Broad Channel|\n",
      "|                 14|     Highbridge Park|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This code presents homework for question 6\n",
    "# Import necessary libraries\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read Parquet data into a DataFrames\n",
    "df_fhv = spark.read.parquet('fhv/2019/10/')\n",
    "df_fhv.registerTempTable('fhv_2019_15')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv('zones/taxi_zone_lookup.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView('zones')\n",
    "# Execute a SQL query using Spark SQL\n",
    "result = spark.sql(\"SELECT * FROM zones LIMIT 10\")\n",
    "#result.show()\n",
    "\n",
    "# Execute a SQL query using Spark SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(fhv.PULocationID), z.Zone\n",
    "FROM \n",
    "    fhv_2019_15 fhv INNER JOIN zones z ON fhv.PULocationID = z.LocationID\n",
    "GROUP BY \n",
    "    z.Zone \n",
    "ORDER BY 1     \n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272d7da-30f7-4dd3-a3aa-b3d97d28be23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
